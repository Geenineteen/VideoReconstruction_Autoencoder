{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "88c47ce565258a22b4c52963f199570a2da8c35a1ec4323c9da01708e0a47c86"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract frames from the mp4 file and preprocess frames\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_and_preprocess_frames(video_path, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert to binary frames\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        (_, binary_frame) = cv2.threshold(gray_frame, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "        cv2.imwrite(os.path.join(output_folder, f\"frame_{frame_count:04d}.png\"), binary_frame)\n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    print(f\"Extracted and converted {frame_count} frames to binary.\")\n",
    "    return frame_count, fps\n",
    "\n",
    "video_path = 'animation.mp4'\n",
    "output_folder = 'binary_frames'\n",
    "frame_count, fps = extract_and_preprocess_frames(video_path, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def prepare_dataset(frame_folder):\n",
    "    frames = sorted([os.path.join(frame_folder, f) for f in os.listdir(frame_folder) if f.endswith('.png')])\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(frames) - 1):\n",
    "        current_frame = cv2.imread(frames[i], cv2.IMREAD_GRAYSCALE)\n",
    "        next_frame = cv2.imread(frames[i + 1], cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        current_frame = np.expand_dims(current_frame, axis=0) / 255.0\n",
    "        next_frame = np.expand_dims(next_frame, axis=0) / 255.0\n",
    "        \n",
    "        X.append(current_frame)\n",
    "        y.append(next_frame)\n",
    "    \n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    y = torch.tensor(y)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X, y = prepare_dataset(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define convolutional autoencoder architecture \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class PredictNextFrame(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PredictNextFrame, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(4, 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 4, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(4, 1, kernel_size=2, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = PredictNextFrame().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    average_loss = 0.0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        average_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {average_loss/len(dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate frames\n",
    "model.eval()\n",
    "\n",
    "generated_frames = [X[0].unsqueeze(0).to(device)]\n",
    "for i in range(1, frame_count - 1):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        next_frame = model(X[i].unsqueeze(0).to(device))\n",
    "        generated_frames.append(next_frame)\n",
    "\n",
    "generated_frames = [frame.squeeze(0).cpu() for frame in generated_frames]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the generated frames back into an MP4 file\n",
    "\n",
    "def frames_to_video(frames, output_path, fps):\n",
    "    height, width = frames[0].shape[1:]\n",
    "    size = (width, height)\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, size, isColor=False)\n",
    "    \n",
    "    for frame in frames:\n",
    "        frame = (frame.numpy().squeeze() * 255).astype(np.uint8)\n",
    "\n",
    "        # Could convert to binary frame to reduce noises\n",
    "        # (_, frame) = cv2.threshold(frame, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "        out.write(frame)\n",
    "    out.release()\n",
    "\n",
    "output_path = 'generated_video.mp4'\n",
    "frames_to_video(generated_frames, output_path, fps)\n",
    "print(f\"Generated video saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": "\n    <video width=\"200\" height=\"200\" controls>\n        <source src=\"generated_video.mp4\" type=\"video/mp4\">\n        Your browser does not support the video tag.\n    </video>\n    "
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Display generated video\n",
    "from IPython.display import HTML\n",
    "\n",
    "def display_video(video_path, width=200, height=200):\n",
    "    video_tag = f'''\n",
    "    <video width=\"{width}\" height=\"{height}\" controls>\n",
    "        <source src=\"{video_path}\" type=\"video/mp4\">\n",
    "        Your browser does not support the video tag.\n",
    "    </video>\n",
    "    '''\n",
    "    return HTML(video_tag)\n",
    "\n",
    "# Display the video\n",
    "display_video('generated_video.mp4')"
   ]
  }
 ]
}